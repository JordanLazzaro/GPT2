# GPT2
From scratch GPT-2 117M for fine-tuning on PG essays (and future datasets)
